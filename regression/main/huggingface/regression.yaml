version: 0.0
name: huggingface-transformers
short: transformers
sources:
    - transformers:
        url: https://github.com/huggingface/transformers.git
        base: transformers
launch:
    path: transformers
    command:
        - python
        - examples/text-classification/run_glue.py
        - --model_name_or_path
        - bert-base-uncased
        - --task_name
        - MRPC
        - --do_train
        - --do_eval
        - --max_seq_length
        - 128
        - --per_gpu_train_batch_size
        - 32
        - --learning_rate
        - 2e-5
        - --max_steps
        - 1
        - --output_dir
        - /tmp/MRPC/
        - --overwrite_output_dir
        - --logging_steps
        - 1
    timeout: 15m
    killtime: 20m
    prep:
        - - python
          - utils/download_glue_data.py
environment:
    pip:
        - git+https://github.com/huggingface/transformers.git
        - datasets
        - sklearn
    variables:
        WANDB_PROJECT: huggingface-demo
        GLUE_DIR: glue_data
        TASK_NAME: MRPC
variants:
    - init:
        - python3s
        - wandb-cli
        - torches1plus

